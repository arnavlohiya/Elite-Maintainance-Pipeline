{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List all .360 files in your Drive\n",
        "!find /content/drive/MyDrive -name \"*.360\" -type f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9K3z92-xsN_",
        "outputId": "d67fb4e9-ef3a-4e2a-e01c-0b1fb776ff39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXH_G1wWugjb",
        "outputId": "d9882fe0-71c1-4a30-f2bc-0666ce432446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,085 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,812 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,288 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,373 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,468 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,851 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,796 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,043 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Fetched 36.7 MB in 5s (7,417 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  libaribb24-0 libavcodec-extra58 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libvo-amrwbenc0\n",
            "Suggested packages:\n",
            "  libcuda1 libnvcuvid1 libnvidia-encode1\n",
            "The following packages will be REMOVED:\n",
            "  libavcodec58\n",
            "The following NEW packages will be installed:\n",
            "  libaribb24-0 libavcodec-extra libavcodec-extra58 libopencore-amrnb0\n",
            "  libopencore-amrwb0 libvo-amrwbenc0\n",
            "0 upgraded, 6 newly installed, 1 to remove and 38 not upgraded.\n",
            "Need to get 5,821 kB of archives.\n",
            "After this operation, 590 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaribb24-0 amd64 1.0.3-2 [26.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libvo-amrwbenc0 amd64 0.1.3-2 [68.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavcodec-extra58 amd64 7:4.4.2-0ubuntu0.22.04.1 [5,566 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavcodec-extra amd64 7:4.4.2-0ubuntu0.22.04.1 [15.2 kB]\n",
            "Fetched 5,821 kB in 1s (4,088 kB/s)\n",
            "Selecting previously unselected package libaribb24-0:amd64.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../libaribb24-0_1.0.3-2_amd64.deb ...\n",
            "Unpacking libaribb24-0:amd64 (1.0.3-2) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libvo-amrwbenc0:amd64.\n",
            "Preparing to unpack .../libvo-amrwbenc0_0.1.3-2_amd64.deb ...\n",
            "Unpacking libvo-amrwbenc0:amd64 (0.1.3-2) ...\n",
            "dpkg: libavcodec58:amd64: dependency problems, but removing anyway as you requested:\n",
            " libopencv-videoio4.5d:amd64 depends on libavcodec58 (>= 7:4.4).\n",
            " libchromaprint1:amd64 depends on libavcodec58 (>= 7:4.4).\n",
            " libavformat58:amd64 depends on libavcodec58 (= 7:4.4.2-0ubuntu0.22.04.1).\n",
            " libavfilter7:amd64 depends on libavcodec58 (= 7:4.4.2-0ubuntu0.22.04.1).\n",
            " libavdevice58:amd64 depends on libavcodec58 (= 7:4.4.2-0ubuntu0.22.04.1).\n",
            " libavcodec-dev:amd64 depends on libavcodec58 (= 7:4.4.2-0ubuntu0.22.04.1).\n",
            " ffmpeg depends on libavcodec58 (= 7:4.4.2-0ubuntu0.22.04.1).\n",
            "\n",
            "(Reading database ... 126699 files and directories currently installed.)\n",
            "Removing libavcodec58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libavcodec-extra58:amd64.\n",
            "(Reading database ... 126693 files and directories currently installed.)\n",
            "Preparing to unpack .../libavcodec-extra58_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libavcodec-extra58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libavcodec-extra:amd64.\n",
            "Preparing to unpack .../libavcodec-extra_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libavcodec-extra:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Setting up libvo-amrwbenc0:amd64 (0.1.3-2) ...\n",
            "Setting up libaribb24-0:amd64 (1.0.3-2) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libavcodec-extra58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Setting up libavcodec-extra:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "libavutil      56. 70.100 / 56. 70.100\n",
            "libavcodec     58.134.100 / 58.134.100\n",
            "libavformat    58. 76.100 / 58. 76.100\n",
            "libavdevice    58. 13.100 / 58. 13.100\n",
            "libavfilter     7.110.100 /  7.110.100\n",
            "libswscale      5.  9.100 /  5.  9.100\n",
            "libswresample   3.  9.100 /  3.  9.100\n",
            "libpostproc    55.  9.100 / 55.  9.100\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python numpy pytesseract\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# Install Tesseract OCR and Python wrapper\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!apt-get update\n",
        "!apt-get install -y ffmpeg libavcodec-extra\n",
        "\n",
        "# Verify installation\n",
        "!ffmpeg -version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Set the correct path\n",
        "video_path = '/content/drive/Shareddrives/Files/360 Videos/Original .360 files/GS012395.360'\n",
        "\n",
        "# 4. Verify the file exists\n",
        "import os\n",
        "if os.path.exists(video_path):\n",
        "    print(f\"✓ File found!\")\n",
        "    print(f\"  Size: {os.path.getsize(video_path) / (1024*1024):.2f} MB\")\n",
        "else:\n",
        "    print(f\"✗ File not found at: {video_path}\")\n",
        "    print(\"\\nTrying to list files in the folder:\")\n",
        "    folder_path = '/content/drive/Shareddrives/Files/360 Videos/Original .360 files'\n",
        "    if os.path.exists(folder_path):\n",
        "        for file in os.listdir(folder_path):\n",
        "            print(f\"  - {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPLMNSDax4HZ",
        "outputId": "9881a607-e336-4e9b-b80e-61f6ee4c61be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✗ File not found at: /content/drive/Shareddrives/Files/360 Videos/Original .360 files/GS012395.360\n",
            "\n",
            "Trying to list files in the folder:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final cell\n",
        "import cv2\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image as IPImage\n",
        "import numpy as np\n",
        "\n",
        "class WhiteboardDetector:\n",
        "    \"\"\"\n",
        "    Detects and extracts whiteboard regions from video frames.\n",
        "    Handles various orientations, partial visibility, and filters out false positives.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, debug=False):\n",
        "        \"\"\"\n",
        "        Initialize the whiteboard detector.\n",
        "\n",
        "        Args:\n",
        "            debug: If True, shows visualization at each stage for debugging\n",
        "        \"\"\"\n",
        "        self.debug = debug\n",
        "\n",
        "        # Configuration parameters for white detection (HSV color space)\n",
        "        # HSV: Hue, Saturation, Value\n",
        "        # For white objects: low saturation, high value\n",
        "        self.white_hsv_lower = np.array([0, 0, 180])      # Lower bound: any hue, low sat, high brightness\n",
        "        self.white_hsv_upper = np.array([180, 50, 255])   # Upper bound: any hue, low sat, max brightness\n",
        "\n",
        "        # Shape filtering parameters\n",
        "        self.min_aspect_ratio = 1.2   # Minimum width:height ratio for whiteboard\n",
        "        self.max_aspect_ratio = 4.0   # Maximum width:height ratio for whiteboard\n",
        "        self.min_area = 5000           # Minimum area in pixels (adjust based on your video resolution)\n",
        "        self.max_area = 500000         # Maximum area in pixels\n",
        "\n",
        "        # Geometry validation parameters\n",
        "        self.boundary_threshold = 20   # Pixels from edge to consider \"touching boundary\"\n",
        "        self.min_solidity = 0.85       # Minimum solidity (filled vs convex hull)\n",
        "\n",
        "        # Edge density parameters (Stage 4)\n",
        "        self.canny_low_threshold = 50    # Lower threshold for Canny edge detection\n",
        "        self.canny_high_threshold = 150  # Upper threshold for Canny edge detection\n",
        "        self.min_edge_density = 0.02     # Minimum ratio of edge pixels to total pixels (2%)\n",
        "                                         # Text on whiteboards creates edges, blank surfaces don't\n",
        "\n",
        "        # Text verification parameters (Stage 5)\n",
        "        self.min_text_confidence = 30    # Minimum OCR confidence score (0-100)\n",
        "        self.min_text_length = 3         # Minimum number of characters detected\n",
        "        self.require_alphanumeric = True # Require at least some alphanumeric characters\n",
        "\n",
        "        # Orientation correction parameters (Stage 8)\n",
        "        self.test_orientations = [0, 90, 180, 270]  # Angles to test for orientation\n",
        "\n",
        "        # Perspective correction parameters (Stage 7)\n",
        "        self.roi_padding = 20            # Extra padding around whiteboard when extracting ROI\n",
        "        self.target_width = 800          # Target width for perspective-corrected whiteboard\n",
        "        self.target_height = 400         # Target height for perspective-corrected whiteboard\n",
        "\n",
        "        # Store intermediate results for visualization\n",
        "        self.white_mask = None\n",
        "        self.all_contours = []\n",
        "        self.filtered_contours = []\n",
        "        self.candidate_boxes = []\n",
        "        self.edge_density_results = []  # Store edge density info for each candidate\n",
        "        self.extracted_rois = []        # Store extracted whiteboard ROIs\n",
        "\n",
        "    def detect_white_regions(self, frame):\n",
        "        \"\"\"\n",
        "        Stage 1: Detect all white regions in the frame using HSV color thresholding.\n",
        "\n",
        "        Args:\n",
        "            frame: Input BGR image from video\n",
        "\n",
        "        Returns:\n",
        "            binary_mask: Binary mask of white regions\n",
        "            contours: List of contours found in white regions\n",
        "        \"\"\"\n",
        "        # Convert from BGR (OpenCV default) to HSV color space\n",
        "        # HSV is better for color-based segmentation than RGB/BGR\n",
        "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # Create binary mask: white pixels become 255, others become 0\n",
        "        # inRange checks if each pixel falls within the HSV bounds\n",
        "        white_mask = cv2.inRange(hsv, self.white_hsv_lower, self.white_hsv_upper)\n",
        "\n",
        "        # Morphological operations to clean up the mask\n",
        "        # 1. Opening: removes small noise (erode then dilate)\n",
        "        # 2. Closing: fills small holes (dilate then erode)\n",
        "        kernel = np.ones((5, 5), np.uint8)\n",
        "        white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "        white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "\n",
        "        # Find contours (boundaries) of white regions\n",
        "        # RETR_EXTERNAL: only get outermost contours (ignore holes)\n",
        "        # CHAIN_APPROX_SIMPLE: compress contours to save memory\n",
        "        contours, _ = cv2.findContours(white_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Store for visualization\n",
        "        self.white_mask = white_mask\n",
        "        self.all_contours = contours\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Stage 1: Found {len(contours)} white regions\")\n",
        "\n",
        "        return white_mask, contours\n",
        "\n",
        "    def filter_by_shape(self, frame, contours):\n",
        "        \"\"\"\n",
        "        Stage 2: Filter contours based on shape characteristics.\n",
        "        Uses minimum area rectangle to handle rotation and perspective.\n",
        "\n",
        "        Args:\n",
        "            frame: Original frame (for getting dimensions)\n",
        "            contours: List of contours to filter\n",
        "\n",
        "        Returns:\n",
        "            filtered_candidates: List of (contour, minAreaRect) tuples that pass filters\n",
        "        \"\"\"\n",
        "        filtered_candidates = []\n",
        "        frame_height, frame_width = frame.shape[:2]\n",
        "\n",
        "        for contour in contours:\n",
        "            # Skip very small contours (noise)\n",
        "            if len(contour) < 5:  # Need at least 5 points for minAreaRect\n",
        "                continue\n",
        "\n",
        "            # Get minimum area rectangle (rotated rectangle that best fits the contour)\n",
        "            # Returns: ((center_x, center_y), (width, height), angle)\n",
        "            # This handles rotation automatically!\n",
        "            rect = cv2.minAreaRect(contour)\n",
        "            (center_x, center_y), (width, height), angle = rect\n",
        "\n",
        "            # Ensure width is always the longer dimension for consistent aspect ratio\n",
        "            if width < height:\n",
        "                width, height = height, width\n",
        "\n",
        "            # Calculate area of the bounding box\n",
        "            area = width * height\n",
        "\n",
        "            # Filter 1: Area must be within reasonable range\n",
        "            if area < self.min_area or area > self.max_area:\n",
        "                if self.debug:\n",
        "                    print(f\"  Rejected: area {area:.0f} outside range [{self.min_area}, {self.max_area}]\")\n",
        "                continue\n",
        "\n",
        "            # Filter 2: Aspect ratio must match typical whiteboard proportions\n",
        "            aspect_ratio = width / height if height > 0 else 0\n",
        "            if aspect_ratio < self.min_aspect_ratio or aspect_ratio > self.max_aspect_ratio:\n",
        "                if self.debug:\n",
        "                    print(f\"  Rejected: aspect ratio {aspect_ratio:.2f} outside range [{self.min_aspect_ratio}, {self.max_aspect_ratio}]\")\n",
        "                continue\n",
        "\n",
        "            # Passed all filters!\n",
        "            filtered_candidates.append((contour, rect))\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"  Accepted: area={area:.0f}, aspect_ratio={aspect_ratio:.2f}, angle={angle:.1f}°\")\n",
        "\n",
        "        # Store for visualization\n",
        "        self.filtered_contours = [c[0] for c in filtered_candidates]\n",
        "        self.candidate_boxes = [c[1] for c in filtered_candidates]\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Stage 2: {len(filtered_candidates)} candidates passed shape filtering\")\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def validate_shape_geometry(self, frame, contour, rect):\n",
        "        \"\"\"\n",
        "        Stage 3: Validate geometry of candidate shapes.\n",
        "        Handles partial whiteboards (touching frame boundary) and checks solidity.\n",
        "\n",
        "        Args:\n",
        "            frame: Original frame (for boundary checking)\n",
        "            contour: Contour to validate\n",
        "            rect: minAreaRect of the contour\n",
        "\n",
        "        Returns:\n",
        "            is_valid: Boolean indicating if shape passes validation\n",
        "            validation_info: Dictionary with validation details\n",
        "        \"\"\"\n",
        "        frame_height, frame_width = frame.shape[:2]\n",
        "\n",
        "        # Approximate contour to polygon\n",
        "        # epsilon: maximum distance from contour to approximated polygon\n",
        "        # Using 2% of perimeter as tolerance\n",
        "        epsilon = 0.02 * cv2.arcLength(contour, True)\n",
        "        approx_polygon = cv2.approxPolyDP(contour, epsilon, True)\n",
        "\n",
        "        num_vertices = len(approx_polygon)\n",
        "\n",
        "        # Check if any vertices are near the image boundary\n",
        "        vertices_on_boundary = 0\n",
        "        for point in approx_polygon:\n",
        "            x, y = point[0]\n",
        "            if (x <= self.boundary_threshold or\n",
        "                x >= frame_width - self.boundary_threshold or\n",
        "                y <= self.boundary_threshold or\n",
        "                y >= frame_height - self.boundary_threshold):\n",
        "                vertices_on_boundary += 1\n",
        "\n",
        "        # Determine if this is a partial whiteboard (cut off by frame edge)\n",
        "        is_partial = vertices_on_boundary > 0\n",
        "\n",
        "        # Validate vertex count\n",
        "        # Complete whiteboard: should have 4 vertices\n",
        "        # Partial whiteboard: may have 3-6 vertices (depending on how it's cut)\n",
        "        valid_vertex_count = False\n",
        "        if is_partial and 3 <= num_vertices <= 6:\n",
        "            valid_vertex_count = True\n",
        "        elif not is_partial and num_vertices == 4:\n",
        "            valid_vertex_count = True\n",
        "\n",
        "        # Check convexity (shape should not have indentations)\n",
        "        is_convex = cv2.isContourConvex(approx_polygon)\n",
        "\n",
        "        # Calculate solidity: ratio of contour area to its convex hull area\n",
        "        # High solidity (close to 1.0) means it's a filled shape, not scattered pixels\n",
        "        contour_area = cv2.contourArea(contour)\n",
        "        hull = cv2.convexHull(contour)\n",
        "        hull_area = cv2.contourArea(hull)\n",
        "        solidity = contour_area / hull_area if hull_area > 0 else 0\n",
        "\n",
        "        # Overall validation\n",
        "        is_valid = (valid_vertex_count and\n",
        "                   (is_convex or is_partial) and  # Allow non-convex if partial\n",
        "                   solidity >= self.min_solidity)\n",
        "\n",
        "        validation_info = {\n",
        "            'num_vertices': num_vertices,\n",
        "            'is_partial': is_partial,\n",
        "            'vertices_on_boundary': vertices_on_boundary,\n",
        "            'is_convex': is_convex,\n",
        "            'solidity': solidity,\n",
        "            'valid_vertex_count': valid_vertex_count\n",
        "        }\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"  Geometry validation:\")\n",
        "            print(f\"    Vertices: {num_vertices} (on boundary: {vertices_on_boundary})\")\n",
        "            print(f\"    Partial: {is_partial}, Convex: {is_convex}, Solidity: {solidity:.2f}\")\n",
        "            print(f\"    Result: {'PASS' if is_valid else 'FAIL'}\")\n",
        "\n",
        "        return is_valid, validation_info\n",
        "\n",
        "    def calculate_edge_density(self, frame, contour, rect):\n",
        "        \"\"\"\n",
        "        Stage 4: Calculate edge density within a candidate region.\n",
        "        Whiteboards with text have high edge density, blank surfaces have low edge density.\n",
        "\n",
        "        Args:\n",
        "            frame: Original frame\n",
        "            contour: Contour of the candidate region\n",
        "            rect: minAreaRect of the contour\n",
        "\n",
        "        Returns:\n",
        "            edge_density: Ratio of edge pixels to total pixels in region\n",
        "            edge_info: Dictionary with detailed edge detection information\n",
        "        \"\"\"\n",
        "        # Create a mask for this specific contour\n",
        "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
        "        cv2.drawContours(mask, [contour], -1, 255, -1)  # Fill the contour\n",
        "\n",
        "        # Convert frame to grayscale for edge detection\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply Canny edge detection on the entire frame\n",
        "        # Canny detects edges based on gradient changes\n",
        "        edges = cv2.Canny(gray, self.canny_low_threshold, self.canny_high_threshold)\n",
        "\n",
        "        # Apply the mask to get edges only within the candidate region\n",
        "        edges_in_region = cv2.bitwise_and(edges, edges, mask=mask)\n",
        "\n",
        "        # Count edge pixels and total pixels in the masked region\n",
        "        edge_pixel_count = np.count_nonzero(edges_in_region)\n",
        "        total_pixel_count = np.count_nonzero(mask)\n",
        "\n",
        "        # Calculate edge density (ratio of edge pixels to total pixels)\n",
        "        edge_density = edge_pixel_count / total_pixel_count if total_pixel_count > 0 else 0\n",
        "\n",
        "        # Determine if this passes the edge density threshold\n",
        "        passes_threshold = edge_density >= self.min_edge_density\n",
        "\n",
        "        edge_info = {\n",
        "            'edge_pixel_count': edge_pixel_count,\n",
        "            'total_pixel_count': total_pixel_count,\n",
        "            'edge_density': edge_density,\n",
        "            'passes_threshold': passes_threshold,\n",
        "            'edges_image': edges_in_region  # Store for visualization\n",
        "        }\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"  Edge density analysis:\")\n",
        "            print(f\"    Edge pixels: {edge_pixel_count}\")\n",
        "            print(f\"    Total pixels: {total_pixel_count}\")\n",
        "            print(f\"    Edge density: {edge_density:.4f} ({edge_density*100:.2f}%)\")\n",
        "            print(f\"    Threshold: {self.min_edge_density:.4f} ({self.min_edge_density*100:.2f}%)\")\n",
        "            print(f\"    Result: {'PASS' if passes_threshold else 'FAIL'}\")\n",
        "\n",
        "        return edge_density, edge_info\n",
        "\n",
        "    def extract_whiteboard_roi(self, frame, contour, rect, validation_info):\n",
        "        \"\"\"\n",
        "        Stage 7: Extract and perspective-correct the whiteboard region.\n",
        "        Handles both complete and partial whiteboards.\n",
        "\n",
        "        Args:\n",
        "            frame: Original frame\n",
        "            contour: Contour of the whiteboard\n",
        "            rect: minAreaRect of the whiteboard\n",
        "            validation_info: Geometry validation information\n",
        "\n",
        "        Returns:\n",
        "            corrected_roi: Perspective-corrected whiteboard image (frontal view)\n",
        "            roi_info: Dictionary with extraction information\n",
        "        \"\"\"\n",
        "        frame_height, frame_width = frame.shape[:2]\n",
        "\n",
        "        # Get the 4 corners of the minimum area rectangle\n",
        "        # boxPoints returns corners in order, but we need them ordered properly\n",
        "        box = cv2.boxPoints(rect)\n",
        "        box = np.float32(box)\n",
        "\n",
        "        # Sort the points to get them in a consistent order:\n",
        "        # [top-left, top-right, bottom-right, bottom-left]\n",
        "        # Method: sort by y-coordinate, then by x-coordinate\n",
        "        box = sorted(box, key=lambda p: (p[1], p[0]))\n",
        "\n",
        "        # Split into top and bottom pairs\n",
        "        top_points = sorted(box[:2], key=lambda p: p[0])  # Top 2, sorted left to right\n",
        "        bottom_points = sorted(box[2:], key=lambda p: p[0])  # Bottom 2, sorted left to right\n",
        "\n",
        "        # Order: top-left, top-right, bottom-right, bottom-left\n",
        "        src_points = np.array([\n",
        "            top_points[0],      # top-left\n",
        "            top_points[1],      # top-right\n",
        "            bottom_points[1],   # bottom-right\n",
        "            bottom_points[0]    # bottom-left\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # Handle partial whiteboards (touching frame boundary)\n",
        "        is_partial = validation_info.get('is_partial', False)\n",
        "\n",
        "        if is_partial:\n",
        "            if self.debug:\n",
        "                print(f\"  Detected partial whiteboard - using bounding rectangle\")\n",
        "\n",
        "            # For partial whiteboards, we can't do perfect perspective correction\n",
        "            # Instead, get the bounding rectangle and extract that region\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "            # Add padding but ensure we stay within frame boundaries\n",
        "            x1 = max(0, x - self.roi_padding)\n",
        "            y1 = max(0, y - self.roi_padding)\n",
        "            x2 = min(frame_width, x + w + self.roi_padding)\n",
        "            y2 = min(frame_height, y + h + self.roi_padding)\n",
        "\n",
        "            # Extract the ROI directly (no perspective transform for partial)\n",
        "            corrected_roi = frame[y1:y2, x1:x2].copy()\n",
        "\n",
        "            roi_info = {\n",
        "                'is_partial': True,\n",
        "                'method': 'bounding_rectangle',\n",
        "                'original_bbox': (x, y, w, h),\n",
        "                'padded_bbox': (x1, y1, x2-x1, y2-y1),\n",
        "                'roi_shape': corrected_roi.shape\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            # Complete whiteboard - apply perspective transform\n",
        "            if self.debug:\n",
        "                print(f\"  Detected complete whiteboard - applying perspective correction\")\n",
        "\n",
        "            # Calculate the width and height of the whiteboard\n",
        "            # Use the distances between corner points\n",
        "            width_top = np.linalg.norm(src_points[1] - src_points[0])\n",
        "            width_bottom = np.linalg.norm(src_points[2] - src_points[3])\n",
        "            width = int(max(width_top, width_bottom))\n",
        "\n",
        "            height_left = np.linalg.norm(src_points[3] - src_points[0])\n",
        "            height_right = np.linalg.norm(src_points[2] - src_points[1])\n",
        "            height = int(max(height_left, height_right))\n",
        "\n",
        "            # Optionally scale to target dimensions while maintaining aspect ratio\n",
        "            aspect_ratio = width / height if height > 0 else 1\n",
        "            if width > self.target_width:\n",
        "                width = self.target_width\n",
        "                height = int(width / aspect_ratio)\n",
        "\n",
        "            # Define destination points for frontal view (rectangle)\n",
        "            dst_points = np.array([\n",
        "                [0, 0],                    # top-left\n",
        "                [width - 1, 0],            # top-right\n",
        "                [width - 1, height - 1],   # bottom-right\n",
        "                [0, height - 1]            # bottom-left\n",
        "            ], dtype=np.float32)\n",
        "\n",
        "            # Compute the perspective transform matrix\n",
        "            transform_matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
        "\n",
        "            # Apply the perspective transformation\n",
        "            corrected_roi = cv2.warpPerspective(frame, transform_matrix, (width, height))\n",
        "\n",
        "            roi_info = {\n",
        "                'is_partial': False,\n",
        "                'method': 'perspective_transform',\n",
        "                'src_points': src_points.tolist(),\n",
        "                'dst_points': dst_points.tolist(),\n",
        "                'original_size': (width, height),\n",
        "                'roi_shape': corrected_roi.shape,\n",
        "                'transform_matrix': transform_matrix\n",
        "            }\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"  ROI extracted:\")\n",
        "            print(f\"    Method: {roi_info['method']}\")\n",
        "            print(f\"    Shape: {roi_info['roi_shape']}\")\n",
        "            print(f\"    Partial: {is_partial}\")\n",
        "\n",
        "        return corrected_roi, roi_info\n",
        "\n",
        "    def verify_text_presence(self, roi):\n",
        "        \"\"\"\n",
        "        Stage 5: Verify that the ROI contains readable text using OCR.\n",
        "        This filters out blank white surfaces and random white objects.\n",
        "\n",
        "        Args:\n",
        "            roi: Extracted region of interest (whiteboard candidate)\n",
        "\n",
        "        Returns:\n",
        "            has_text: Boolean indicating if meaningful text was found\n",
        "            text_info: Dictionary with OCR results and details\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import pytesseract\n",
        "        except ImportError:\n",
        "            if self.debug:\n",
        "                print(\"  Warning: pytesseract not installed. Skipping text verification.\")\n",
        "                print(\"  Install with: !pip install pytesseract\")\n",
        "            return True, {'skipped': True, 'reason': 'pytesseract not available'}\n",
        "\n",
        "        # Convert to grayscale for better OCR\n",
        "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply preprocessing to improve OCR accuracy\n",
        "        # 1. Apply adaptive thresholding to handle varying lighting\n",
        "        binary = cv2.adaptiveThreshold(\n",
        "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "            cv2.THRESH_BINARY, 11, 2\n",
        "        )\n",
        "\n",
        "        # 2. Optional: denoise\n",
        "        denoised = cv2.fastNlMeansDenoising(binary, None, 10, 7, 21)\n",
        "\n",
        "        # Run OCR with detailed output including confidence scores\n",
        "        # PSM 6: Assume a single uniform block of text\n",
        "        # PSM 11: Sparse text - find as much text as possible in no particular order\n",
        "        try:\n",
        "            ocr_data = pytesseract.image_to_data(\n",
        "                denoised,\n",
        "                output_type=pytesseract.Output.DICT,\n",
        "                config='--psm 6'  # Try single block first\n",
        "            )\n",
        "        except Exception as e:\n",
        "            if self.debug:\n",
        "                print(f\"  OCR Error: {e}\")\n",
        "            return False, {'error': str(e)}\n",
        "\n",
        "        # Extract detected text and confidence scores\n",
        "        detected_text = []\n",
        "        confidences = []\n",
        "\n",
        "        for i, conf in enumerate(ocr_data['conf']):\n",
        "            # Filter out low confidence detections (-1 means no text detected)\n",
        "            if int(conf) > 0:\n",
        "                text = ocr_data['text'][i].strip()\n",
        "                if text:  # Not empty\n",
        "                    detected_text.append(text)\n",
        "                    confidences.append(int(conf))\n",
        "\n",
        "        # Combine all detected text\n",
        "        full_text = ' '.join(detected_text)\n",
        "\n",
        "        # Calculate metrics\n",
        "        num_chars = len(full_text.replace(' ', ''))\n",
        "        num_alphanumeric = sum(c.isalnum() for c in full_text)\n",
        "        avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
        "\n",
        "        # Determine if this passes text verification\n",
        "        has_sufficient_length = num_chars >= self.min_text_length\n",
        "        has_sufficient_confidence = avg_confidence >= self.min_text_confidence\n",
        "        has_alphanumeric = num_alphanumeric > 0 if self.require_alphanumeric else True\n",
        "\n",
        "        passes_verification = (has_sufficient_length and\n",
        "                              has_sufficient_confidence and\n",
        "                              has_alphanumeric)\n",
        "\n",
        "        text_info = {\n",
        "            'detected_text': full_text,\n",
        "            'num_characters': num_chars,\n",
        "            'num_alphanumeric': num_alphanumeric,\n",
        "            'avg_confidence': avg_confidence,\n",
        "            'individual_confidences': confidences,\n",
        "            'passes_verification': passes_verification,\n",
        "            'has_sufficient_length': has_sufficient_length,\n",
        "            'has_sufficient_confidence': has_sufficient_confidence,\n",
        "            'has_alphanumeric': has_alphanumeric\n",
        "        }\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"  Text verification:\")\n",
        "            print(f\"    Detected: '{full_text}'\")\n",
        "            print(f\"    Characters: {num_chars} (alphanumeric: {num_alphanumeric})\")\n",
        "            print(f\"    Avg Confidence: {avg_confidence:.1f}%\")\n",
        "            print(f\"    Checks: length={has_sufficient_length}, conf={has_sufficient_confidence}, alnum={has_alphanumeric}\")\n",
        "            print(f\"    Result: {'PASS' if passes_verification else 'FAIL'}\")\n",
        "\n",
        "        return passes_verification, text_info\n",
        "\n",
        "    def correct_orientation(self, roi):\n",
        "        \"\"\"\n",
        "        Stage 8: Detect and correct the orientation of the whiteboard.\n",
        "        Tests OCR at 0°, 90°, 180°, 270° and selects the best orientation.\n",
        "\n",
        "        Args:\n",
        "            roi: Extracted whiteboard ROI\n",
        "\n",
        "        Returns:\n",
        "            corrected_roi: ROI rotated to correct orientation\n",
        "            orientation_info: Dictionary with rotation details\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import pytesseract\n",
        "        except ImportError:\n",
        "            if self.debug:\n",
        "                print(\"  Warning: pytesseract not available. Skipping orientation correction.\")\n",
        "            return roi, {'skipped': True, 'best_angle': 0}\n",
        "\n",
        "        best_confidence = -1\n",
        "        best_angle = 0\n",
        "        best_text = \"\"\n",
        "        orientation_results = {}\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"  Testing orientations: {self.test_orientations}\")\n",
        "\n",
        "        for angle in self.test_orientations:\n",
        "            # Rotate the image\n",
        "            if angle == 0:\n",
        "                rotated = roi.copy()\n",
        "            elif angle == 90:\n",
        "                rotated = cv2.rotate(roi, cv2.ROTATE_90_CLOCKWISE)\n",
        "            elif angle == 180:\n",
        "                rotated = cv2.rotate(roi, cv2.ROTATE_180)\n",
        "            elif angle == 270:\n",
        "                rotated = cv2.rotate(roi, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "            else:\n",
        "                rotated = roi.copy()\n",
        "\n",
        "            # Preprocess for OCR\n",
        "            gray = cv2.cvtColor(rotated, cv2.COLOR_BGR2GRAY)\n",
        "            binary = cv2.adaptiveThreshold(\n",
        "                gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                cv2.THRESH_BINARY, 11, 2\n",
        "            )\n",
        "            denoised = cv2.fastNlMeansDenoising(binary, None, 10, 7, 21)\n",
        "\n",
        "            # Run OCR\n",
        "            try:\n",
        "                ocr_data = pytesseract.image_to_data(\n",
        "                    denoised,\n",
        "                    output_type=pytesseract.Output.DICT,\n",
        "                    config='--psm 6'\n",
        "                )\n",
        "\n",
        "                # Calculate average confidence for this orientation\n",
        "                confidences = [int(conf) for conf in ocr_data['conf'] if int(conf) > 0]\n",
        "                avg_conf = sum(confidences) / len(confidences) if confidences else 0\n",
        "\n",
        "                # Extract text\n",
        "                text_parts = [ocr_data['text'][i].strip() for i, conf in enumerate(ocr_data['conf'])\n",
        "                             if int(conf) > 0 and ocr_data['text'][i].strip()]\n",
        "                detected_text = ' '.join(text_parts)\n",
        "\n",
        "                orientation_results[angle] = {\n",
        "                    'confidence': avg_conf,\n",
        "                    'text': detected_text,\n",
        "                    'num_chars': len(detected_text.replace(' ', ''))\n",
        "                }\n",
        "\n",
        "                if self.debug:\n",
        "                    print(f\"    {angle:3d}°: confidence={avg_conf:5.1f}%, text='{detected_text}'\")\n",
        "\n",
        "                # Update best orientation based on confidence\n",
        "                if avg_conf > best_confidence:\n",
        "                    best_confidence = avg_conf\n",
        "                    best_angle = angle\n",
        "                    best_text = detected_text\n",
        "\n",
        "            except Exception as e:\n",
        "                if self.debug:\n",
        "                    print(f\"    {angle:3d}°: OCR failed - {e}\")\n",
        "                orientation_results[angle] = {\n",
        "                    'confidence': 0,\n",
        "                    'text': '',\n",
        "                    'error': str(e)\n",
        "                }\n",
        "\n",
        "        # Apply the best rotation\n",
        "        if best_angle == 0:\n",
        "            corrected_roi = roi.copy()\n",
        "        elif best_angle == 90:\n",
        "            corrected_roi = cv2.rotate(roi, cv2.ROTATE_90_CLOCKWISE)\n",
        "        elif best_angle == 180:\n",
        "            corrected_roi = cv2.rotate(roi, cv2.ROTATE_180)\n",
        "        elif best_angle == 270:\n",
        "            corrected_roi = cv2.rotate(roi, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "        else:\n",
        "            corrected_roi = roi.copy()\n",
        "\n",
        "        orientation_info = {\n",
        "            'best_angle': best_angle,\n",
        "            'best_confidence': best_confidence,\n",
        "            'best_text': best_text,\n",
        "            'all_results': orientation_results\n",
        "        }\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"  Selected orientation: {best_angle}° (confidence: {best_confidence:.1f}%)\")\n",
        "            print(f\"  Best text: '{best_text}'\")\n",
        "\n",
        "        return corrected_roi, orientation_info\n",
        "\n",
        "    def visualize_detection_stages(self, frame, validated_candidates_with_edges=None, extracted_rois=None):\n",
        "        \"\"\"\n",
        "        Visualize the results of each detection stage for debugging.\n",
        "\n",
        "        Args:\n",
        "            frame: Original frame\n",
        "            validated_candidates_with_edges: List of candidates that passed edge density\n",
        "            extracted_rois: List of extracted and corrected ROIs\n",
        "        \"\"\"\n",
        "        # Determine grid size based on whether we have ROIs\n",
        "        if extracted_rois:\n",
        "            fig, axes = plt.subplots(3, 3, figsize=(18, 16))\n",
        "        else:\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Stage 1: White mask\n",
        "        axes[0, 0].imshow(self.white_mask, cmap='gray')\n",
        "        axes[0, 0].set_title(f'Stage 1: White Regions ({len(self.all_contours)} found)')\n",
        "        axes[0, 0].axis('off')\n",
        "\n",
        "        # Stage 1: Contours on original\n",
        "        frame_contours = frame.copy()\n",
        "        cv2.drawContours(frame_contours, self.all_contours, -1, (0, 255, 0), 2)\n",
        "        axes[0, 1].imshow(cv2.cvtColor(frame_contours, cv2.COLOR_BGR2RGB))\n",
        "        axes[0, 1].set_title('Stage 1: All White Contours')\n",
        "        axes[0, 1].axis('off')\n",
        "\n",
        "        # Stage 2: Filtered by shape\n",
        "        frame_filtered = frame.copy()\n",
        "        cv2.drawContours(frame_filtered, self.filtered_contours, -1, (0, 255, 0), 3)\n",
        "        # Draw bounding boxes\n",
        "        for rect in self.candidate_boxes:\n",
        "            box = cv2.boxPoints(rect)\n",
        "            box = np.intp(box)\n",
        "            cv2.drawContours(frame_filtered, [box], 0, (255, 0, 0), 2)\n",
        "        axes[0, 2].imshow(cv2.cvtColor(frame_filtered, cv2.COLOR_BGR2RGB))\n",
        "        axes[0, 2].set_title(f'Stage 2: Shape Filtered ({len(self.filtered_contours)} candidates)')\n",
        "        axes[0, 2].axis('off')\n",
        "\n",
        "        # Stage 3: Geometry validated (before edge density)\n",
        "        frame_geometry = frame.copy()\n",
        "        if self.edge_density_results:\n",
        "            for result in self.edge_density_results:\n",
        "                contour = result['contour']\n",
        "                cv2.drawContours(frame_geometry, [contour], -1, (255, 165, 0), 3)  # Orange for geometry pass\n",
        "        axes[1, 0].imshow(cv2.cvtColor(frame_geometry, cv2.COLOR_BGR2RGB))\n",
        "        axes[1, 0].set_title(f'Stage 3: Geometry Validated ({len(self.edge_density_results)} candidates)')\n",
        "        axes[1, 0].axis('off')\n",
        "\n",
        "        # Stage 4: Edge density visualization (combined edges)\n",
        "        if self.edge_density_results:\n",
        "            # Create a combined edge map showing all candidates\n",
        "            combined_edges = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
        "            for result in self.edge_density_results:\n",
        "                edges_img = result['edge_info']['edges_image']\n",
        "                combined_edges = cv2.bitwise_or(combined_edges, edges_img)\n",
        "            axes[1, 1].imshow(combined_edges, cmap='gray')\n",
        "            axes[1, 1].set_title('Stage 4: Edge Detection in Candidates')\n",
        "        else:\n",
        "            axes[1, 1].imshow(np.zeros(frame.shape[:2]), cmap='gray')\n",
        "            axes[1, 1].set_title('Stage 4: No Candidates')\n",
        "        axes[1, 1].axis('off')\n",
        "\n",
        "        # Stage 4: Final candidates after edge density filtering\n",
        "        frame_final = frame.copy()\n",
        "        if validated_candidates_with_edges:\n",
        "            for candidate in validated_candidates_with_edges:\n",
        "                contour = candidate[0]\n",
        "                edge_density = candidate[3]  # edge_density value\n",
        "                # Draw in bright green for final candidates\n",
        "                cv2.drawContours(frame_final, [contour], -1, (0, 255, 0), 4)\n",
        "\n",
        "                # Add edge density text\n",
        "                M = cv2.moments(contour)\n",
        "                if M[\"m00\"] != 0:\n",
        "                    cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "                    cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "                    cv2.putText(frame_final, f\"{edge_density*100:.1f}%\",\n",
        "                               (cX-50, cY), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                               1.5, (0, 255, 0), 3)\n",
        "        axes[1, 2].imshow(cv2.cvtColor(frame_final, cv2.COLOR_BGR2RGB))\n",
        "        axes[1, 2].set_title(f'Stage 4: Edge Density Filtered ({len(validated_candidates_with_edges) if validated_candidates_with_edges else 0} final)')\n",
        "        axes[1, 2].axis('off')\n",
        "\n",
        "        # Stage 7: Extracted ROIs (if available)\n",
        "        if extracted_rois:\n",
        "            for idx, roi_data in enumerate(extracted_rois[:3]):  # Show up to 3 ROIs\n",
        "                roi = roi_data['roi']\n",
        "                roi_info = roi_data['roi_info']\n",
        "\n",
        "                axes[2, idx].imshow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
        "                title = f\"Stage 7: Extracted ROI {idx+1}\\n\"\n",
        "                title += f\"Method: {roi_info['method']}\\n\"\n",
        "                title += f\"Shape: {roi_info['roi_shape'][:2]}\"\n",
        "                axes[2, idx].set_title(title, fontsize=10)\n",
        "                axes[2, idx].axis('off')\n",
        "\n",
        "            # Hide unused ROI slots\n",
        "            for idx in range(len(extracted_rois), 3):\n",
        "                axes[2, idx].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        \"\"\"\n",
        "        Main processing pipeline: detect whiteboard in a single frame.\n",
        "        Implements Stages 1-5, 7-8.\n",
        "\n",
        "        Args:\n",
        "            frame: Input BGR frame from video\n",
        "\n",
        "        Returns:\n",
        "            results: List of detection results\n",
        "                    Each result is a dictionary containing:\n",
        "                    - 'contour': The whiteboard contour\n",
        "                    - 'rect': minAreaRect\n",
        "                    - 'validation_info': Geometry validation details\n",
        "                    - 'edge_density': Edge density value\n",
        "                    - 'edge_info': Edge detection details\n",
        "                    - 'roi': Extracted, orientation-corrected whiteboard image\n",
        "                    - 'roi_info': ROI extraction details\n",
        "                    - 'orientation_info': Orientation correction details\n",
        "                    - 'text_info': OCR verification results\n",
        "        \"\"\"\n",
        "        if self.debug:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"WHITEBOARD DETECTION PIPELINE\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "        # Stage 1: Detect white regions\n",
        "        white_mask, contours = self.detect_white_regions(frame)\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            if self.debug:\n",
        "                print(\"No white regions found!\")\n",
        "            return []\n",
        "\n",
        "        # Stage 2: Filter by shape\n",
        "        shape_candidates = self.filter_by_shape(frame, contours)\n",
        "\n",
        "        if len(shape_candidates) == 0:\n",
        "            if self.debug:\n",
        "                print(\"No candidates passed shape filtering!\")\n",
        "            return []\n",
        "\n",
        "        # Stage 3: Validate geometry\n",
        "        geometry_validated = []\n",
        "        for contour, rect in shape_candidates:\n",
        "            is_valid, validation_info = self.validate_shape_geometry(frame, contour, rect)\n",
        "            if is_valid:\n",
        "                geometry_validated.append((contour, rect, validation_info))\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nStage 3: {len(geometry_validated)} candidates passed geometry validation\")\n",
        "\n",
        "        if len(geometry_validated) == 0:\n",
        "            if self.debug:\n",
        "                print(\"No candidates passed geometry validation!\")\n",
        "            return []\n",
        "\n",
        "        # Stage 4: Edge density analysis\n",
        "        if self.debug:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(\"STAGE 4: EDGE DENSITY ANALYSIS\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "        edge_density_validated = []\n",
        "        self.edge_density_results = []  # Reset for visualization\n",
        "\n",
        "        for contour, rect, validation_info in geometry_validated:\n",
        "            edge_density, edge_info = self.calculate_edge_density(frame, contour, rect)\n",
        "\n",
        "            # Store for visualization (all candidates that reached this stage)\n",
        "            self.edge_density_results.append({\n",
        "                'contour': contour,\n",
        "                'rect': rect,\n",
        "                'validation_info': validation_info,\n",
        "                'edge_density': edge_density,\n",
        "                'edge_info': edge_info\n",
        "            })\n",
        "\n",
        "            # Filter by edge density threshold\n",
        "            if edge_info['passes_threshold']:\n",
        "                edge_density_validated.append((contour, rect, validation_info, edge_density, edge_info))\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nStage 4: {len(edge_density_validated)} candidates passed edge density filtering\")\n",
        "\n",
        "        if len(edge_density_validated) == 0:\n",
        "            if self.debug:\n",
        "                print(\"No candidates passed edge density filtering!\")\n",
        "                self.visualize_detection_stages(frame, [])\n",
        "            return []\n",
        "\n",
        "        # Stage 7: Extract and perspective-correct ROIs\n",
        "        if self.debug:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(\"STAGE 7: PERSPECTIVE CORRECTION & ROI EXTRACTION\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "        extracted_rois = []\n",
        "        self.extracted_rois = []  # Reset for visualization\n",
        "\n",
        "        for contour, rect, validation_info, edge_density, edge_info in edge_density_validated:\n",
        "            # Extract the whiteboard ROI\n",
        "            roi, roi_info = self.extract_whiteboard_roi(frame, contour, rect, validation_info)\n",
        "\n",
        "            extracted_rois.append((contour, rect, validation_info, edge_density, edge_info, roi, roi_info))\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nStage 7: Successfully extracted {len(extracted_rois)} whiteboard ROI(s)\")\n",
        "\n",
        "        if len(extracted_rois) == 0:\n",
        "            if self.debug:\n",
        "                self.visualize_detection_stages(frame, edge_density_validated, [])\n",
        "            return []\n",
        "\n",
        "        # Stage 8: Orientation Correction (before text verification for better accuracy)\n",
        "        if self.debug:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(\"STAGE 8: ORIENTATION CORRECTION\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "        orientation_corrected = []\n",
        "\n",
        "        for contour, rect, validation_info, edge_density, edge_info, roi, roi_info in extracted_rois:\n",
        "            # Correct the orientation\n",
        "            corrected_roi, orientation_info = self.correct_orientation(roi)\n",
        "\n",
        "            # Store the corrected ROI for visualization\n",
        "            self.extracted_rois.append({\n",
        "                'roi': corrected_roi,\n",
        "                'roi_info': roi_info\n",
        "            })\n",
        "\n",
        "            orientation_corrected.append((contour, rect, validation_info, edge_density, edge_info, corrected_roi, roi_info, orientation_info))\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nStage 8: Corrected orientation for {len(orientation_corrected)} ROI(s)\")\n",
        "\n",
        "        # Stage 5: Text Pattern Verification (done after orientation correction)\n",
        "        if self.debug:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(\"STAGE 5: TEXT PATTERN VERIFICATION\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "        text_verified = []\n",
        "\n",
        "        for contour, rect, validation_info, edge_density, edge_info, roi, roi_info, orientation_info in orientation_corrected:\n",
        "            # Verify text presence in the corrected ROI\n",
        "            has_text, text_info = self.verify_text_presence(roi)\n",
        "\n",
        "            # Only keep candidates with verified text\n",
        "            if has_text:\n",
        "                # Package all information together\n",
        "                result = {\n",
        "                    'contour': contour,\n",
        "                    'rect': rect,\n",
        "                    'validation_info': validation_info,\n",
        "                    'edge_density': edge_density,\n",
        "                    'edge_info': edge_info,\n",
        "                    'roi': roi,  # This is the orientation-corrected ROI\n",
        "                    'roi_info': roi_info,\n",
        "                    'orientation_info': orientation_info,\n",
        "                    'text_info': text_info\n",
        "                }\n",
        "\n",
        "                text_verified.append(result)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nStage 5: {len(text_verified)} candidate(s) passed text verification\")\n",
        "            # Visualize with orientation-corrected ROIs\n",
        "            self.visualize_detection_stages(frame, edge_density_validated, self.extracted_rois)\n",
        "\n",
        "        return text_verified\n",
        "\n",
        "\n",
        "class VideoFrameExtractor:\n",
        "    \"\"\"\n",
        "    Extract frames from .360 videos for whiteboard ID detection.\n",
        "    Now integrated with WhiteboardDetector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, video_path, output_dir=\"extracted_frames\", debug_detection=False):\n",
        "        \"\"\"\n",
        "        Initialize the frame extractor.\n",
        "\n",
        "        Args:\n",
        "            video_path: Path to the .360 video file\n",
        "            output_dir: Directory to save extracted frames\n",
        "            debug_detection: Enable debug mode for whiteboard detection\n",
        "        \"\"\"\n",
        "        self.video_path = video_path\n",
        "        self.output_dir = output_dir\n",
        "        self.frames = []\n",
        "        self.whiteboard_detector = WhiteboardDetector(debug=debug_detection)\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def extract_frames(self, duration_seconds=100, fps_sample=10):\n",
        "        \"\"\"\n",
        "        Extract frames from the first N seconds of the video.\n",
        "\n",
        "        Args:\n",
        "            duration_seconds: How many seconds to extract from (default: 100)\n",
        "            fps_sample: Sample rate - extract 1 frame per N seconds (default: 10)\n",
        "\n",
        "        Returns:\n",
        "            List of extracted frame paths\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(self.video_path)\n",
        "        print(f\"Sample rate is:{fps_sample}\")\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Error: Could not open video file {self.video_path}. \"\n",
        "                           f\"The file exists but OpenCV cannot read it. \"\n",
        "                           f\"Try installing: !apt-get install -y ffmpeg\")\n",
        "\n",
        "        # Get video properties\n",
        "        video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        print(f\"Video Properties:\")\n",
        "        print(f\"  - FPS: {video_fps}\")\n",
        "        print(f\"  - Total Frames: {total_frames}\")\n",
        "        print(f\"  - Resolution: {width}x{height}\")\n",
        "        print(f\"  - Duration: {total_frames/video_fps:.2f} seconds\")\n",
        "\n",
        "        # Calculate frame interval for sampling\n",
        "        frame_interval = max(1, int(video_fps * fps_sample))\n",
        "        max_frame = min(int(video_fps * duration_seconds), total_frames)\n",
        "\n",
        "        print(f\"\\nExtracting frames:\")\n",
        "        print(f\"  - Sampling every {frame_interval} frames (≈{fps_sample}s intervals)\")\n",
        "        print(f\"  - Max frame index: {max_frame}\")\n",
        "\n",
        "        frame_count = 0\n",
        "        extracted_count = 0\n",
        "        extracted_paths = []\n",
        "        consecutive_failures = 0\n",
        "        max_consecutive_failures = 10  # Stop if we can't read 10 frames in a row\n",
        "\n",
        "        while cap.isOpened() and frame_count < max_frame:\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            # Handle read failures - try to skip problematic frames\n",
        "            if not ret:\n",
        "                consecutive_failures += 1\n",
        "                print(f\"  Warning: Failed to read frame {frame_count} (consecutive failures: {consecutive_failures})\")\n",
        "\n",
        "                # If too many consecutive failures, stop processing\n",
        "                if consecutive_failures >= max_consecutive_failures:\n",
        "                    print(f\"\\n*** Too many consecutive failures ({consecutive_failures}). Stopping extraction. ***\")\n",
        "                    print(f\"*** Successfully extracted {extracted_count} frames before failure. ***\")\n",
        "                    break\n",
        "\n",
        "                # Try to skip to next frame\n",
        "                frame_count += 1\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
        "                continue\n",
        "\n",
        "            # Reset consecutive failures on successful read\n",
        "            consecutive_failures = 0\n",
        "\n",
        "            # Validate frame is not empty\n",
        "            if frame is None or frame.size == 0:\n",
        "                print(f\"  Warning: Frame {frame_count} is empty, skipping\")\n",
        "                frame_count += 1\n",
        "                continue\n",
        "\n",
        "            # Extract frame at specified intervals\n",
        "            if frame_count % frame_interval == 0:\n",
        "                timestamp = frame_count / video_fps\n",
        "                frame_filename = f\"frame_{extracted_count:03d}_t{timestamp:.2f}s.jpg\"\n",
        "                frame_path = os.path.join(self.output_dir, frame_filename)\n",
        "\n",
        "                # Save frame\n",
        "                success = cv2.imwrite(frame_path, frame)\n",
        "                if success:\n",
        "                    extracted_paths.append(frame_path)\n",
        "                    self.frames.append(frame)\n",
        "                    print(f\"  Extracted: {frame_filename}\")\n",
        "                    extracted_count += 1\n",
        "                else:\n",
        "                    print(f\"  Error: Failed to write {frame_filename}\")\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        print(f\"\\nTotal frames extracted: {extracted_count}\")\n",
        "        return extracted_paths\n",
        "\n",
        "    def display_frames(self, num_frames=6, figsize=(15, 10)):\n",
        "        \"\"\"\n",
        "        Display extracted frames in a grid for manual review.\n",
        "\n",
        "        Args:\n",
        "            num_frames: Number of frames to display (default: 6)\n",
        "            figsize: Figure size for matplotlib (default: (15, 10))\n",
        "        \"\"\"\n",
        "        if not self.frames:\n",
        "            print(\"No frames extracted yet. Run extract_frames() first.\")\n",
        "            return\n",
        "\n",
        "        num_frames = min(num_frames, len(self.frames))\n",
        "        cols = 3\n",
        "        rows = (num_frames + cols - 1) // cols\n",
        "\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "\n",
        "        if rows == 1 and cols == 1:\n",
        "            axes = np.array([axes])\n",
        "        else:\n",
        "            axes = axes.flatten()\n",
        "\n",
        "        for idx in range(num_frames):\n",
        "            frame = self.frames[idx]\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            axes[idx].imshow(frame_rgb)\n",
        "            axes[idx].set_title(f\"Frame {idx}\")\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "        for idx in range(num_frames, len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def detect_whiteboards_in_frames(self, frame_indices=None):\n",
        "        \"\"\"\n",
        "        Run whiteboard detection on extracted frames.\n",
        "\n",
        "        Args:\n",
        "            frame_indices: List of frame indices to process, or None to process all\n",
        "\n",
        "        Returns:\n",
        "            detection_results: Dictionary mapping frame_index to detection results\n",
        "        \"\"\"\n",
        "        if not self.frames:\n",
        "            print(\"No frames extracted yet. Run extract_frames() first.\")\n",
        "            return {}\n",
        "\n",
        "        if frame_indices is None:\n",
        "            frame_indices = range(len(self.frames))\n",
        "\n",
        "        detection_results = {}\n",
        "\n",
        "        for idx in frame_indices:\n",
        "            if idx >= len(self.frames):\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Processing Frame {idx}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            frame = self.frames[idx]\n",
        "            candidates = self.whiteboard_detector.process_frame(frame)\n",
        "            detection_results[idx] = candidates\n",
        "\n",
        "            print(f\"Frame {idx}: Found {len(candidates)} whiteboard candidate(s)\")\n",
        "\n",
        "        return detection_results\n",
        "\n",
        "    def get_frame_info(self):\n",
        "        \"\"\"\n",
        "        Get information about extracted frames.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with frame information\n",
        "        \"\"\"\n",
        "        if not self.frames:\n",
        "            return {\"message\": \"No frames extracted yet\"}\n",
        "\n",
        "        return {\n",
        "            \"total_frames\": len(self.frames),\n",
        "            \"frame_shape\": self.frames[0].shape if self.frames else None,\n",
        "            \"output_directory\": self.output_dir\n",
        "        }\n",
        "\n",
        "\n",
        "# Example usage function\n",
        "def process_video(video_path, duration=7, sample_rate=1, display_count=6, debug_detection=False):\n",
        "    \"\"\"\n",
        "    Complete workflow to extract and display frames from a video.\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to the .360 video file\n",
        "        duration: Seconds to extract from start of video (default: 10)\n",
        "        sample_rate: Extract 1 frame per N seconds (default: 10)\n",
        "        display_count: Number of frames to display (default: 6)\n",
        "        debug_detection: Enable whiteboard detection debugging (default: False)\n",
        "    \"\"\"\n",
        "    print(f\"Processing video: {video_path}\\n\")\n",
        "\n",
        "    # Create extractor instance with whiteboard detection\n",
        "    extractor = VideoFrameExtractor(video_path, debug_detection=debug_detection)\n",
        "\n",
        "    # Extract frames\n",
        "    frame_paths = extractor.extract_frames(\n",
        "        duration_seconds=duration,\n",
        "        fps_sample=sample_rate\n",
        "    )\n",
        "\n",
        "    # Display frames\n",
        "    print(\"\\nDisplaying frames for manual review...\")\n",
        "    extractor.display_frames(num_frames=display_count)\n",
        "\n",
        "    # Print summary\n",
        "    info = extractor.get_frame_info()\n",
        "    print(f\"\\nFrame Info:\")\n",
        "    print(f\"  - Total extracted: {info['total_frames']}\")\n",
        "    print(f\"  - Frame dimensions: {info['frame_shape']}\")\n",
        "    print(f\"  - Saved to: {info['output_directory']}\")\n",
        "\n",
        "    return extractor, frame_paths\n",
        "\n",
        "\n",
        "# Usage example with whiteboard detection:\n",
        "extractor, paths = process_video(\"GS012395.360\", duration=7, sample_rate=1, debug_detection=True)\n",
        "results = extractor.detect_whiteboards_in_frames(frame_indices=[0, 1, 2])  # Test on first 3 frames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "HWIi8VsPumJZ",
        "outputId": "88d31a47-dc24-49a2-9ed6-f22908d1ce58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing video: GS012395.360\n",
            "\n",
            "Sample rate is:1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Error: Could not open video file GS012395.360. The file exists but OpenCV cannot read it. Try installing: !apt-get install -y ffmpeg",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3856595647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;31m# Usage example with whiteboard detection:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m \u001b[0mextractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GS012395.360\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_detection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_whiteboards_in_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Test on first 3 frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3856595647.py\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(video_path, duration, sample_rate, display_count, debug_detection)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;31m# Extract frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m     frame_paths = extractor.extract_frames(\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0mduration_seconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mfps_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3856595647.py\u001b[0m in \u001b[0;36mextract_frames\u001b[0;34m(self, duration_seconds, fps_sample)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             raise ValueError(f\"Error: Could not open video file {self.video_path}. \"\n\u001b[0m\u001b[1;32m    951\u001b[0m                            \u001b[0;34mf\"The file exists but OpenCV cannot read it. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m                            f\"Try installing: !apt-get install -y ffmpeg\")\n",
            "\u001b[0;31mValueError\u001b[0m: Error: Could not open video file GS012395.360. The file exists but OpenCV cannot read it. Try installing: !apt-get install -y ffmpeg"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRoQGRZTupJY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}